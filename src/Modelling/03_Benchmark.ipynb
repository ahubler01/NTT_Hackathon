{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from prophet import Prophet\n",
    "import timesfm\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --------------------------\n",
    "# Required Packages for Models\n",
    "# --------------------------\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# PyTorch and related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "cmdstanpy_logger = logging.getLogger(\"cmdstanpy\")\n",
    "absl_logger = logging.getLogger(\"absl\")\n",
    "cmdstanpy_logger.disabled = True\n",
    "absl_logger.disabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files: 100%|██████████| 3/3 [00:00<00:00, 26379.27it/s]\n"
     ]
    }
   ],
   "source": [
    "tfm = timesfm.TimesFm(\n",
    "      hparams=timesfm.TimesFmHparams(\n",
    "          backend=\"gpu\",\n",
    "          per_core_batch_size=32,\n",
    "          horizon_len=128,\n",
    "          num_layers=50,\n",
    "          use_positional_embedding=False,\n",
    "          context_len=2048,\n",
    "      ),\n",
    "      checkpoint=timesfm.TimesFmCheckpoint(\n",
    "          huggingface_repo_id=\"google/timesfm-2.0-500m-pytorch\"),\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omie_b = pd.read_csv('../../data/df_omie_blind(in).csv')\n",
    "df_omie_l = pd.read_csv('../../data/df_omie_labelled(in).csv')\n",
    "filtered_cat = pd.read_csv('../../data/filtered_categories(in).csv')\n",
    "unit_list = pd.read_csv('../../data/unit_list(in).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_omie_l.merge(unit_list, on='Codigo', how='left')\n",
    "codes = filtered_cat['Codigo'].unique()\n",
    "data = data[data['Codigo'].isin(codes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 703248 entries, 0 to 712263\n",
      "Data columns (total 11 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   Codigo                703248 non-null  object \n",
      " 1   Descripcion           703248 non-null  object \n",
      " 2   fechaHora             703248 non-null  object \n",
      " 3   PrecEuro              703248 non-null  float64\n",
      " 4   Energia               703248 non-null  float64\n",
      " 5   Descripción           403466 non-null  object \n",
      " 6   Agente                403466 non-null  object \n",
      " 7   Porcentaje_Propiedad  403466 non-null  float64\n",
      " 8   Tipo_Unidad           403466 non-null  object \n",
      " 9   Zona/Frontera         403466 non-null  object \n",
      " 10  Tecnología            403466 non-null  object \n",
      "dtypes: float64(3), object(8)\n",
      "memory usage: 64.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Codigo'] = data['Codigo'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['fechaHora'] = pd.to_datetime(data['fechaHora'])\n",
    "data['cum_energy'] = data.groupby(['Codigo', 'fechaHora'])['Energia'].cumsum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_features(df: pd.DataFrame):\n",
    "    df['fechaHora'] = pd.to_datetime(df['fechaHora'])\n",
    "    df['date'] = df['fechaHora'].dt.date\n",
    "    df['hour'] = df['fechaHora'].dt.hour\n",
    "    df['day_of_week'] = df['fechaHora'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "    df['month'] = df['fechaHora'].dt.month\n",
    "    df['day_of_month'] = df['fechaHora'].dt.day\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    df.sort_values(['fechaHora', 'Codigo'], inplace=True)\n",
    "    df['t'] = (df['fechaHora'] - df['fechaHora'].min()).dt.total_seconds() / 3600\n",
    "    \n",
    "    def add_fourier_terms(df: pd.DataFrame, period, K, time_col='t'):\n",
    "        df = df.sort_values(['Codigo', 'fechaHora'])\n",
    "        for k in range(1, K + 1):\n",
    "            df[f'sin_{period}_{k}'] = np.sin(2 * np.pi * k * df[time_col] / period)\n",
    "            df[f'cos_{period}_{k}'] = np.cos(2 * np.pi * k * df[time_col] / period)\n",
    "        return df\n",
    "    \n",
    "    df = add_fourier_terms(df, period=24, K=3)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def cyclical_features(df: pd.DataFrame):\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    return df\n",
    "    \n",
    "def agg_features(df: pd.DataFrame):\n",
    "    df['cum_energy'] = df.groupby(['Codigo', 'date'])['Energia'].cumsum()\n",
    "    return df\n",
    "        \n",
    "def interaction_features(df: pd.DataFrame):\n",
    "    df['energia_hour_sin'] = df['Energia'] * df['hour_sin']\n",
    "    return df\n",
    "    \n",
    "def lags_features(df: pd.DataFrame):\n",
    "    df.sort_values(['fechaHora'], inplace=True)\n",
    "    df['PrecEuro'] = df.groupby('Codigo')['PrecEuro'].shift(24*28)\n",
    "    df['lag_Energia'] = df.groupby('Codigo')['Energia'].shift(24*28)\n",
    "    df['lag_Energia'] = np.log(df['lag_Energia'] + 1)\n",
    "    df['lag1_Energia'] = df.groupby('Codigo')['lag_Energia'].shift(1)\n",
    "    df['lag24_Energia'] = df.groupby('Codigo')['lag_Energia'].shift(24)\n",
    "    return df\n",
    "    \n",
    "def rolling_mean_features(df: pd.DataFrame):\n",
    "    df['roll24_mean_Energia'] = df.groupby('Codigo')['Energia'] \\\n",
    "        .transform(lambda x: x.rolling(window=24, min_periods=1).mean())\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(data: pd.DataFrame):\n",
    "    data = time_features(data)\n",
    "    data = cyclical_features(data)\n",
    "    data = agg_features(data)\n",
    "    data = interaction_features(data)\n",
    "    data = lags_features(data)\n",
    "    data = rolling_mean_features(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 703248 entries, 1 to 712104\n",
      "Data columns (total 34 columns):\n",
      " #   Column                Non-Null Count   Dtype         \n",
      "---  ------                --------------   -----         \n",
      " 0   Codigo                703247 non-null  object        \n",
      " 1   Descripcion           703248 non-null  object        \n",
      " 2   fechaHora             703248 non-null  datetime64[ns]\n",
      " 3   PrecEuro              493583 non-null  float64       \n",
      " 4   Energia               703248 non-null  float64       \n",
      " 5   Descripción           403466 non-null  object        \n",
      " 6   Agente                403466 non-null  object        \n",
      " 7   Porcentaje_Propiedad  403466 non-null  float64       \n",
      " 8   Tipo_Unidad           403466 non-null  object        \n",
      " 9   Zona/Frontera         403466 non-null  object        \n",
      " 10  Tecnología            403466 non-null  object        \n",
      " 11  cum_energy            703247 non-null  float64       \n",
      " 12  date                  703248 non-null  object        \n",
      " 13  hour                  703248 non-null  int32         \n",
      " 14  day_of_week           703248 non-null  int32         \n",
      " 15  month                 703248 non-null  int32         \n",
      " 16  day_of_month          703248 non-null  int32         \n",
      " 17  is_weekend            703248 non-null  int64         \n",
      " 18  t                     703248 non-null  float64       \n",
      " 19  sin_24_1              703248 non-null  float64       \n",
      " 20  cos_24_1              703248 non-null  float64       \n",
      " 21  sin_24_2              703248 non-null  float64       \n",
      " 22  cos_24_2              703248 non-null  float64       \n",
      " 23  sin_24_3              703248 non-null  float64       \n",
      " 24  cos_24_3              703248 non-null  float64       \n",
      " 25  hour_sin              703248 non-null  float64       \n",
      " 26  hour_cos              703248 non-null  float64       \n",
      " 27  dow_sin               703248 non-null  float64       \n",
      " 28  dow_cos               703248 non-null  float64       \n",
      " 29  energia_hour_sin      703248 non-null  float64       \n",
      " 30  lag_Energia           493583 non-null  float64       \n",
      " 31  lag1_Energia          493271 non-null  float64       \n",
      " 32  lag24_Energia         486095 non-null  float64       \n",
      " 33  roll24_mean_Energia   703247 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(20), int32(4), int64(1), object(8)\n",
      "memory usage: 177.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data = feature_engineering(data)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values(['fechaHora', 'Codigo'])\n",
    "\n",
    "# Create a log-transformed and first-differenced (stationary) series.\n",
    "data['Energia_stationary'] = data['Energia'].diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to lags, we have many missing values at the beginning of each codigo. As it represents a large proportion of the df, I will impute by the mean per hour. This might cause leakages for the first records but shouldn't impact the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PrecEuro                209665\n",
      "Descripción             299782\n",
      "Agente                  299782\n",
      "Porcentaje_Propiedad    299782\n",
      "Tipo_Unidad             299782\n",
      "Zona/Frontera           299782\n",
      "Tecnología              299782\n",
      "lag_Energia             209665\n",
      "lag1_Energia            209977\n",
      "lag24_Energia           217153\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nan_counts = data.isna().sum()\n",
    "nan_counts = nan_counts[nan_counts > 1]\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop Technologia, Zona/Frontera, Tipo_Unidad, Porcentaje_Propiedad, Agente and Descripción as their feature importance is close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['Descripción', 'Agente', 'Porcentaje_Propiedad', 'Tipo_Unidad', 'Zona/Frontera', 'Tecnología'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_codigo_hour(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Impute missing values in selected columns using the mean computed per Codigo and per hour.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    df['fechaHora'] = pd.to_datetime(df['fechaHora'])\n",
    "\n",
    "    cols_to_impute = ['PrecEuro', 'lag_Energia', 'lag1_Energia', 'lag24_Energia']\n",
    "\n",
    "    for col in cols_to_impute:\n",
    "        df[col] = df.groupby(['Codigo', 'hour'])[col].transform(lambda x: x.fillna(x.mean()))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = impute_codigo_hour(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OrdinalEncoder()\n",
    "cat_cols=data.select_dtypes(include=['object']).columns\n",
    "data[cat_cols] = encoder.fit_transform(data[cat_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TimesFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimesFMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(TimesFMModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_timesfm(X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame) -> tuple:\n",
    "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "    return X_train_tensor, y_train_tensor, X_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESFM_MODEL_PARAMS = {\n",
    "    'hidden_dim': 100,\n",
    "    'output_dim': 1,\n",
    "}\n",
    "\n",
    "TIMESFM_TRAIN_PARAMS = {\n",
    "    'lr': 0.01,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_timesfm(\n",
    "    X_train_tensor: torch.Tensor, \n",
    "    y_train_tensor: torch.Tensor,\n",
    "    TIMESFM_MODEL_PARAMS: dict,\n",
    "    TIMESFM_TRAIN_PARAMS: dict,\n",
    "    epochs: int = 150\n",
    "    ) -> TimesFMModel:\n",
    "\n",
    "    timesfm_model = TimesFMModel(input_dim=len(FEATURES), **TIMESFM_MODEL_PARAMS)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(timesfm_model.parameters(), **TIMESFM_TRAIN_PARAMS)\n",
    "    \n",
    "    # Train the model.\n",
    "    timesfm_model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = timesfm_model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return timesfm_model\n",
    "    \n",
    "def evaluate_timesfm(timesfm_model: TimesFMModel, X_test_tensor: torch.Tensor) -> np.ndarray:\n",
    "    timesfm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_timesfm = timesfm_model(X_test_tensor).numpy().flatten()\n",
    "        \n",
    "    return y_pred_timesfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # batch_first=True makes input shape (batch, seq, feature)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states with zeros (and send to same device as x)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        # Use the output from the final time step\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lstm(X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame) -> tuple:\n",
    "    selected_features = [\n",
    "        'lag1_Energia', 'lag24_Energia', 'roll24_mean_Energia',\n",
    "        'hour', 'day_of_week', 'day_of_month', 'month', 'is_weekend',\n",
    "        'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos',\n",
    "        'PrecEuro', 'cum_energy'\n",
    "    ]\n",
    "    X_train_selected = X_train[selected_features].values  # shape: (num_samples, num_features)\n",
    "    X_test_selected  = X_test[selected_features].values\n",
    "\n",
    "    # Add a time dimension (sequence length = 1)\n",
    "    X_train_lstm = torch.tensor(X_train_selected, dtype=torch.float32).unsqueeze(1)  # shape: (samples, 1, num_features)\n",
    "    X_test_lstm  = torch.tensor(X_test_selected, dtype=torch.float32).unsqueeze(1)\n",
    "    y_train_lstm = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "    \n",
    "    return X_train_lstm, y_train_lstm, X_test_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_MODEL_PARAMS = {\n",
    "    'hidden_size': 50,\n",
    "    'num_layers': 1,\n",
    "    'output_size': 1\n",
    "}\n",
    "\n",
    "LSTM_TRAIN_PARAMS = {\n",
    "    'lr': 0.01,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(\n",
    "    X_train: torch.Tensor, \n",
    "    y_train: torch.Tensor, \n",
    "    LSTM_MODEL_PARAMS: dict,\n",
    "    LSTM_TRAIN_PARAMS: dict,\n",
    "    epochs: int=150\n",
    "    ) -> LSTMModel:\n",
    "    \n",
    "    input_size = X_train.shape[2]\n",
    "    lstm_model = LSTMModel(input_size, **LSTM_MODEL_PARAMS)\n",
    "\n",
    "    criterion_lstm = nn.MSELoss()\n",
    "    optimizer_lstm = optim.Adam(lstm_model.parameters(), **LSTM_TRAIN_PARAMS)\n",
    "    \n",
    "    lstm_model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer_lstm.zero_grad()\n",
    "        outputs = lstm_model(X_train)\n",
    "        loss = criterion_lstm(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer_lstm.step()\n",
    "        \n",
    "    return lstm_model\n",
    "    \n",
    "    \n",
    "def evaluate_lstm(lstm_model: LSTMModel, X_test_lstm: torch.Tensor) -> np.ndarray:\n",
    "    lstm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_lstm = lstm_model(X_test_lstm).numpy().flatten()\n",
    "    return y_pred_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_PARAMS = {\n",
    "    'n_estimators': 1000,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.01,\n",
    "    'verbosity': 0,\n",
    "    'booster': 'gbtree'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgb(\n",
    "    X_train: pd.DataFrame, \n",
    "    y_train: pd.Series, \n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    XGB_PARAMS: dict) -> xgb.XGBRegressor:    \n",
    "    xgb_reg = xgb.XGBRegressor(**XGB_PARAMS)\n",
    "    xgb_reg.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        verbose=False\n",
    "    )\n",
    "    return xgb_reg\n",
    "    \n",
    "def evaluate_xgb(xgb_reg: xgb.XGBRegressor, X_test: pd.DataFrame) -> np.ndarray:\n",
    "    return xgb_reg.predict(X_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBM_PARAMS = {\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm(\n",
    "    X_train: pd.DataFrame, \n",
    "    y_train: pd.Series, \n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    LGBM_PARAMS: dict\n",
    "    ) -> LGBMRegressor:    \n",
    "    lgbm_reg = LGBMRegressor(**LGBM_PARAMS)\n",
    "    lgbm_reg.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)]\n",
    "    )\n",
    "    return lgbm_reg\n",
    "    \n",
    "def evaluate_lgbm(lgbm_reg: LGBMRegressor, X_test: pd.DataFrame) -> np.ndarray:\n",
    "    return lgbm_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prophet(train: pd.DataFrame) -> pd.DataFrame:\n",
    "    train_prophet = train.reset_index()\n",
    "    train_prophet['ds'] = pd.to_datetime(train_prophet['fechaHora'])\n",
    "\n",
    "    train_prophet = train_prophet.rename(columns={TARGET: 'y'})\n",
    "\n",
    "    return train_prophet[['ds', 'y', 'PrecEuro', 'cum_energy']]\n",
    "\n",
    "def train_prophet(train_prophet: pd.DataFrame) -> Prophet:\n",
    "\n",
    "    prophet_model = Prophet()\n",
    "    prophet_model.add_regressor('PrecEuro')\n",
    "    prophet_model.add_regressor('cum_energy')\n",
    "\n",
    "    # Fit the model\n",
    "    prophet_model.fit(train_prophet)\n",
    "    \n",
    "    return prophet_model\n",
    "\n",
    "def evaluate_prophet(prophet_model: Prophet, test: pd.DataFrame) -> np.ndarray:\n",
    "\n",
    "    test_prophet = test.reset_index()\n",
    "    test_prophet['ds'] = pd.to_datetime(test_prophet['fechaHora'])\n",
    "    future = test_prophet[['ds', 'PrecEuro', 'cum_energy']]\n",
    "\n",
    "    # Forecast\n",
    "    forecast = prophet_model.predict(future)\n",
    "    return forecast['yhat'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Rolling Window CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_rolling_window_cv(data: pd.DataFrame, initial_train_window: int, forecast_horizon: int, step: int):\n",
    "    \"\"\"\n",
    "    Custom rolling window cross-validation.\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    train_end = initial_train_window  \n",
    "    while (train_end + forecast_horizon) <= n:\n",
    "        train_idx = list(range(0, train_end))\n",
    "        test_idx = list(range(train_end, train_end + forecast_horizon))\n",
    "        yield train_idx, test_idx\n",
    "        train_end += step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nunique_codes = data['Codigo'].nunique()\n",
    "INITIAL_TRAIN_WINDOW = 24*28*nunique_codes\n",
    "FORECAST_HORIZON = 24*28*nunique_codes\n",
    "STEP = 24*7*nunique_codes\n",
    "\n",
    "EXCLUDED_COLS = ['fechaHora', 'Energia']\n",
    "FEATURES = [col for col in data.columns if col not in EXCLUDED_COLS]\n",
    "TARGET = 'Energia_stationary'\n",
    "\n",
    "model_names = ['TimesFM', 'XGB', 'LGBM', 'Prophet', 'LSTM']\n",
    "results = {model: {'mae': [], 'mape': []} for model in model_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_score(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "def mape_score(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / np.abs(y_true+ 1e-3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 0 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet    --> MAE: 131.3310, MAPE: 20855.1568\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[20:09:13] /Users/runner/work/xgboost/xgboost/src/data/data.cc:514: Check failed: valid: Label contains NaN, infinity or a value too large.\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x000000030763858c dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x00000003077964ec xgboost::MetaInfo::SetInfoFromHost(xgboost::Context const&, xgboost::StringView, xgboost::Json) + 2776\n  [bt] (2) 3   libxgboost.dylib                    0x0000000307795888 xgboost::MetaInfo::SetInfo(xgboost::Context const&, xgboost::StringView, xgboost::StringView) + 464\n  [bt] (3) 4   libxgboost.dylib                    0x000000030764fbc4 XGDMatrixSetInfoFromInterface + 228\n  [bt] (4) 5   libffi.8.dylib                      0x000000010830804c ffi_call_SYSV + 76\n  [bt] (5) 6   libffi.8.dylib                      0x0000000108305834 ffi_call_int + 1404\n  [bt] (6) 7   _ctypes.cpython-311-darwin.so       0x0000000106fd8134 _ctypes_callproc + 752\n  [bt] (7) 8   _ctypes.cpython-311-darwin.so       0x0000000106fd248c PyCFuncPtr_call + 228\n  [bt] (8) 9   python3.11                          0x0000000104dc96c4 _PyEval_EvalFrameDefault + 192680\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[564], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# --------------------------\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGB\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_names:\n\u001b[0;32m---> 30\u001b[0m     xgb_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_xgb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXGB_PARAMS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     y_pred_xgb \u001b[38;5;241m=\u001b[39m evaluate_xgb(xgb_model, X_test)\n\u001b[1;32m     33\u001b[0m     mae_xgb \u001b[38;5;241m=\u001b[39m mae_score(y_test, y_pred_xgb)\n",
      "Cell \u001b[0;32mIn[553], line 8\u001b[0m, in \u001b[0;36mtrain_xgb\u001b[0;34m(X_train, y_train, X_test, y_test, XGB_PARAMS)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_xgb\u001b[39m(\n\u001b[1;32m      2\u001b[0m     X_train: pd\u001b[38;5;241m.\u001b[39mDataFrame, \n\u001b[1;32m      3\u001b[0m     y_train: pd\u001b[38;5;241m.\u001b[39mSeries, \n\u001b[1;32m      4\u001b[0m     X_test: pd\u001b[38;5;241m.\u001b[39mDataFrame,\n\u001b[1;32m      5\u001b[0m     y_test: pd\u001b[38;5;241m.\u001b[39mSeries,\n\u001b[1;32m      6\u001b[0m     XGB_PARAMS: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor:    \n\u001b[1;32m      7\u001b[0m     xgb_reg \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mXGB_PARAMS)\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mxgb_reg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xgb_reg\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/xgboost/sklearn.py:1081\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbosity):\n\u001b[1;32m   1080\u001b[0m     evals_result: TrainingCallback\u001b[38;5;241m.\u001b[39mEvalsLog \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1081\u001b[0m     train_dmatrix, evals \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_evaluation_matrices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_qid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_dmatrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/xgboost/sklearn.py:596\u001b[0m, in \u001b[0;36m_wrap_evaluation_matrices\u001b[0;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_wrap_evaluation_matrices\u001b[39m(\n\u001b[1;32m    577\u001b[0m     missing: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    578\u001b[0m     X: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    592\u001b[0m     feature_types: Optional[FeatureTypes],\n\u001b[1;32m    593\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, List[Tuple[Any, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;124;03m    way.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 596\u001b[0m     train_dmatrix \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dmatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m     n_validation \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_set \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(eval_set)\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalidate_or_none\u001b[39m(meta: Optional[Sequence], name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/xgboost/sklearn.py:1003\u001b[0m, in \u001b[0;36mXGBModel._create_dmatrix\u001b[0;34m(self, ref, **kwargs)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _can_use_qdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_method) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbooster \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1003\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQuantileDMatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnthread\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_bin\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/xgboost/core.py:1573\u001b[0m, in \u001b[0;36mQuantileDMatrix.__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, data_split_mode)\u001b[0m\n\u001b[1;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m   1554\u001b[0m         info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1555\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m         )\n\u001b[1;32m   1567\u001b[0m     ):\n\u001b[1;32m   1568\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1569\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf data iterator is used as input, data like label should be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1570\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified as batch argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1571\u001b[0m         )\n\u001b[0;32m-> 1573\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/xgboost/core.py:1632\u001b[0m, in \u001b[0;36mQuantileDMatrix._init\u001b[0;34m(self, data, ref, enable_categorical, **meta)\u001b[0m\n\u001b[1;32m   1620\u001b[0m config \u001b[38;5;241m=\u001b[39m make_jcargs(\n\u001b[1;32m   1621\u001b[0m     nthread\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnthread, missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing, max_bin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_bin\n\u001b[1;32m   1622\u001b[0m )\n\u001b[1;32m   1623\u001b[0m ret \u001b[38;5;241m=\u001b[39m _LIB\u001b[38;5;241m.\u001b[39mXGQuantileDMatrixCreateFromCallback(\n\u001b[1;32m   1624\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1625\u001b[0m     it\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1630\u001b[0m     ctypes\u001b[38;5;241m.\u001b[39mbyref(handle),\n\u001b[1;32m   1631\u001b[0m )\n\u001b[0;32m-> 1632\u001b[0m \u001b[43mit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;66;03m# delay check_call to throw intermediate exception first\u001b[39;00m\n\u001b[1;32m   1634\u001b[0m _check_call(ret)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/xgboost/core.py:569\u001b[0m, in \u001b[0;36mDataIter.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    567\u001b[0m exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/xgboost/core.py:550\u001b[0m, in \u001b[0;36mDataIter._handle_exception\u001b[0;34m(self, fn, dft_ret)\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dft_ret\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;66;03m# Defer the exception in order to return 0 and stop the iteration.\u001b[39;00m\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;66;03m# Exception inside a ctype callback function has no effect except\u001b[39;00m\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;66;03m# for printing to stderr (doesn't stop the execution).\u001b[39;00m\n\u001b[1;32m    555\u001b[0m     tb \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/xgboost/core.py:637\u001b[0m, in \u001b[0;36mDataIter._next_wrapper.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_temporary_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_exception(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/xgboost/data.py:1416\u001b[0m, in \u001b[0;36mSingleBatchInternalIter.next\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m   1414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1415\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mit \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1416\u001b[0m \u001b[43minput_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/xgboost/core.py:626\u001b[0m, in \u001b[0;36mDataIter._next_wrapper.<locals>.input_data\u001b[0;34m(data, feature_names, feature_types, **kwargs)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_temporary_data \u001b[38;5;241m=\u001b[39m (new, cat_codes, feature_names, feature_types)\n\u001b[1;32m    625\u001b[0m dispatch_proxy_set_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxy, new, cat_codes)\n\u001b[0;32m--> 626\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_ref \u001b[38;5;241m=\u001b[39m ref\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/xgboost/core.py:954\u001b[0m, in \u001b[0;36mDMatrix.set_info\u001b[0;34m(self, label, weight, base_margin, group, qid, label_lower_bound, label_upper_bound, feature_names, feature_types, feature_weights)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dispatch_meta_backend\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_weight(weight)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/xgboost/core.py:1092\u001b[0m, in \u001b[0;36mDMatrix.set_label\u001b[0;34m(self, label)\u001b[0m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Set label of dmatrix\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m \n\u001b[1;32m   1085\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;124;03m    The label information to be set into DMatrix\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dispatch_meta_backend\n\u001b[0;32m-> 1092\u001b[0m \u001b[43mdispatch_meta_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/xgboost/data.py:1362\u001b[0m, in \u001b[0;36mdispatch_meta_backend\u001b[0;34m(matrix, data, name, dtype)\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pandas_series(data):\n\u001b[0;32m-> 1362\u001b[0m     \u001b[43m_meta_from_pandas_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_dlpack(data):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/xgboost/data.py:679\u001b[0m, in \u001b[0;36m_meta_from_pandas_series\u001b[0;34m(data, name, dtype, handle)\u001b[0m\n\u001b[1;32m    677\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto_dense()  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 679\u001b[0m \u001b[43m_meta_from_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/xgboost/data.py:1295\u001b[0m, in \u001b[0;36m_meta_from_numpy\u001b[0;34m(data, field, dtype, handle)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMasked array is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1294\u001b[0m interface_str \u001b[38;5;241m=\u001b[39m _array_interface(data)\n\u001b[0;32m-> 1295\u001b[0m \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGDMatrixSetInfoFromInterface\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterface_str\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/xgboost/core.py:284\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [20:09:13] /Users/runner/work/xgboost/xgboost/src/data/data.cc:514: Check failed: valid: Label contains NaN, infinity or a value too large.\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x000000030763858c dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x00000003077964ec xgboost::MetaInfo::SetInfoFromHost(xgboost::Context const&, xgboost::StringView, xgboost::Json) + 2776\n  [bt] (2) 3   libxgboost.dylib                    0x0000000307795888 xgboost::MetaInfo::SetInfo(xgboost::Context const&, xgboost::StringView, xgboost::StringView) + 464\n  [bt] (3) 4   libxgboost.dylib                    0x000000030764fbc4 XGDMatrixSetInfoFromInterface + 228\n  [bt] (4) 5   libffi.8.dylib                      0x000000010830804c ffi_call_SYSV + 76\n  [bt] (5) 6   libffi.8.dylib                      0x0000000108305834 ffi_call_int + 1404\n  [bt] (6) 7   _ctypes.cpython-311-darwin.so       0x0000000106fd8134 _ctypes_callproc + 752\n  [bt] (7) 8   _ctypes.cpython-311-darwin.so       0x0000000106fd248c PyCFuncPtr_call + 228\n  [bt] (8) 9   python3.11                          0x0000000104dc96c4 _PyEval_EvalFrameDefault + 192680\n\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_idx, test_idx) in enumerate(custom_rolling_window_cv(data, INITIAL_TRAIN_WINDOW, FORECAST_HORIZON, STEP)):\n",
    "    print(f\"\\n===== Fold {fold} =====\")\n",
    "    \n",
    "    # Split the data into train and test folds\n",
    "    train = data.iloc[train_idx]\n",
    "    test = data.iloc[test_idx]\n",
    "    \n",
    "    # Common splits for models that require X and y inputs:\n",
    "    X_train = train[FEATURES]\n",
    "    y_train = train[TARGET]\n",
    "    X_test  = test[FEATURES]\n",
    "    y_test  = test[TARGET]\n",
    "    \n",
    "    # --------------------------\n",
    "    if 'Prophet' in model_names:\n",
    "        data_prophet = prepare_prophet(train)\n",
    "        prophet_model = train_prophet(data_prophet)\n",
    "        y_pred_prophet = evaluate_prophet(prophet_model, test)\n",
    "        \n",
    "        mae_prophet = mae_score(y_test, y_pred_prophet)\n",
    "        mape_prophet = mape_score(y_test, y_pred_prophet)\n",
    "        \n",
    "        results['Prophet']['mae'].append(mae_score(y_test, y_pred_prophet))\n",
    "        results['Prophet']['mape'].append(mape_prophet)\n",
    "        \n",
    "        print(f\"Prophet    --> MAE: {mae_prophet:.4f}, MAPE: {mape_prophet:.4f}\")\n",
    "    \n",
    "    # --------------------------\n",
    "    if 'XGB' in model_names:\n",
    "        xgb_model = train_xgb(X_train, y_train, X_test, y_test, XGB_PARAMS)\n",
    "        y_pred_xgb = evaluate_xgb(xgb_model, X_test)\n",
    "        \n",
    "        mae_xgb = mae_score(y_test, y_pred_xgb)\n",
    "        mape_xgb = mape_score(y_test, y_pred_xgb)\n",
    "        \n",
    "        results['XGB']['mae'].append(mae_xgb)\n",
    "        results['XGB']['mape'].append(mape_xgb)\n",
    "        \n",
    "        print(f\"XGBoost    --> MAE: {mae_xgb:.4f}, MAPE: {mape_xgb:.4f}\")\n",
    "    \n",
    "    # --------------------------\n",
    "    if 'LGBM' in model_names:\n",
    "        lgbm_model = train_lgbm(X_train, y_train,  X_test, y_test, LGBM_PARAMS)\n",
    "        y_pred_lgbm = evaluate_lgbm(lgbm_model, X_test)\n",
    "\n",
    "        mae_lgbm = mae_score(y_test, y_pred_lgbm)\n",
    "        mape_lgbm = mape_score(y_test, y_pred_lgbm)\n",
    "\n",
    "        results['LGBM']['mae'].append(mae_lgbm)\n",
    "        results['LGBM']['mape'].append(mape_lgbm)\n",
    "\n",
    "        print(f\"LightGBM   --> MAE: {mae_lgbm:.4f}, MAPE: {mape_lgbm:.4f}\")\n",
    "\n",
    "    # --------------------------\n",
    "    if 'TimesFM' in model_names:\n",
    "        X_train_timesfm, y_train_timesfm, X_test_timesfm = prepare_timesfm(X_train, y_train, X_test)  \n",
    "        timesfm_model = train_timesfm(X_train_timesfm, y_train_timesfm, TIMESFM_MODEL_PARAMS, TIMESFM_TRAIN_PARAMS)\n",
    "        y_pred_timesfm = evaluate_timesfm(timesfm_model, X_test_timesfm)\n",
    "        \n",
    "        mae_timesfm = mae_score(y_test, y_pred_timesfm)\n",
    "        mape_timesfm = mape_score(y_test, y_pred_timesfm)\n",
    "        \n",
    "        results['TimesFM']['mae'].append(mae_timesfm)\n",
    "        results['TimesFM']['mape'].append(mape_timesfm)\n",
    "        \n",
    "        print(f\"TimesFM   --> MAE: {mae_timesfm:.4f}, MAPE: {mape_timesfm:.4f}\")\n",
    "    \n",
    "    # --------------------------\n",
    "    # LSTM Model (PyTorch)\n",
    "    # --------------------------\n",
    "    if 'LSTM' in model_names:\n",
    "        X_train_lstm, y_train_lstm, X_test_timesfm = prepare_lstm(X_train, y_train, X_test)\n",
    "        lstm_model = train_lstm(X_train_lstm, y_train_lstm, LSTM_MODEL_PARAMS, LSTM_TRAIN_PARAMS)\n",
    "        y_pred_lstm = evaluate_lstm(lstm_model, X_test_timesfm)\n",
    "        \n",
    "        mae_lstm = mae_score(y_test, y_pred_lstm)\n",
    "        mape_lstm = mape_score(y_test, y_pred_lstm)\n",
    "        \n",
    "        results['LSTM']['mae'].append(mae_lstm)\n",
    "        results['LSTM']['mape'].append(mape_lstm)\n",
    "        \n",
    "        print(f\"LSTM       --> MAE: {mae_lstm:.4f}, MAPE: {mape_lstm:.4f}\")\n",
    "    \n",
    "# --------------------------\n",
    "# STEP 5: Aggregate and Display Results\n",
    "# --------------------------\n",
    "benchmark_results = []\n",
    "for model in model_names:\n",
    "    avg_mae = np.mean(results[model]['mae'])\n",
    "    avg_mape = np.mean(results[model]['mape'])\n",
    "    benchmark_results.append({\n",
    "        'Model': model,\n",
    "        'MAE': avg_mae,\n",
    "        'MAPE': avg_mape\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(benchmark_results)\n",
    "print(\"\\n===== Benchmark Results =====\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost    --> MAE: 5.3114, MAPE: 7.1780\n"
     ]
    }
   ],
   "source": [
    "XGB_PARAMS = {\n",
    "    'n_estimators': 1000,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.01,\n",
    "    'verbosity': 0,\n",
    "    'booster': 'gbtree'\n",
    "}\n",
    "\n",
    "# Convert 'fechaHora' column to datetime\n",
    "data['fechaHora'] = pd.to_datetime(data['fechaHora'])\n",
    "\n",
    "# Split the data such that the test set represents the last 3 months\n",
    "split_date = pd.to_datetime('2024-05-01')\n",
    "train = data[data['fechaHora'] < split_date]\n",
    "test = data[data['fechaHora'] >= split_date]\n",
    "\n",
    "# Common splits for models that require X and y inputs:\n",
    "X_train = train[FEATURES]\n",
    "y_train = train[TARGET]\n",
    "X_test  = test[FEATURES]\n",
    "y_test  = test[TARGET]\n",
    "\n",
    "xgb_model = train_xgb(X_train, y_train, X_test, y_test, XGB_PARAMS)\n",
    "y_pred_xgb = evaluate_xgb(xgb_model, X_test)\n",
    "\n",
    "mae_xgb = mae_score(y_test, y_pred_xgb)\n",
    "mape_xgb = mape_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"XGBoost    --> MAE: {mae_xgb:.4f}, MAPE: {mape_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gain-based importances: {'Codigo': 15071271.0, 'PrecEuro': 3375221.5, 'cum_energy': 18128722.0, 'date': 15013311.0, 'day_of_week': 12675636.0, 'day_of_month': 26607256.0, 'sin_24_1': 8660682.0, 'sin_24_3': 5884109.5, 'lag_Energia': 13829793.0, 'lag1_Energia': 15316444.0, 'lag24_Energia': 13615051.0, 'roll24_mean_Energia': 40523412.0, 'Energia_stationary': 161339488.0}\n",
      "Cover-based importances: {'Codigo': 1625.7435302734375, 'PrecEuro': 158.0, 'cum_energy': 17416.72265625, 'date': 1014.9596557617188, 'day_of_week': 1021.686767578125, 'day_of_month': 865.9361572265625, 'sin_24_1': 424.2727355957031, 'sin_24_3': 426.79998779296875, 'lag_Energia': 514.4390258789062, 'lag1_Energia': 460.0, 'lag24_Energia': 14483.6611328125, 'roll24_mean_Energia': 23202.5390625, 'Energia_stationary': 83649.890625}\n"
     ]
    }
   ],
   "source": [
    "booster = xgb_model.get_booster()\n",
    "gain_importances = booster.get_score(importance_type='gain')\n",
    "cover_importances = booster.get_score(importance_type='cover')\n",
    "print(\"Gain-based importances:\", gain_importances)\n",
    "print(\"Cover-based importances:\", cover_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04305581, 0.        , 0.00964238, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05179037,\n",
       "       0.04289022, 0.        , 0.03621192, 0.        , 0.07601196,\n",
       "       0.        , 0.        , 0.02474195, 0.        , 0.        ,\n",
       "       0.        , 0.0168098 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.03950914, 0.04375622,\n",
       "       0.03889566, 0.11576782, 0.4609168 ], dtype=float32)"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.feature_importances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importancias basadas en gain: {'Codigo': 15071271.0, 'PrecEuro': 3375221.5, 'cum_energy': 18128722.0, 'date': 15013311.0, 'day_of_week': 12675636.0, 'day_of_month': 26607256.0, 'sin_24_1': 8660682.0, 'sin_24_3': 5884109.5, 'lag_Energia': 13829793.0, 'lag1_Energia': 15316444.0, 'lag24_Energia': 13615051.0, 'roll24_mean_Energia': 40523412.0, 'Energia_stationary': 161339488.0}\n"
     ]
    }
   ],
   "source": [
    "booster = xgb_model.get_booster()\n",
    "gain_importances = booster.get_score(importance_type='gain')\n",
    "print(\"Importancias basadas en gain:\", gain_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Feature  Importance\n",
      "32    Energia_stationary    0.460917\n",
      "31   roll24_mean_Energia    0.115768\n",
      "14          day_of_month    0.076012\n",
      "9             cum_energy    0.051790\n",
      "29          lag1_Energia    0.043756\n",
      "0                 Codigo    0.043056\n",
      "10                  date    0.042890\n",
      "28           lag_Energia    0.039509\n",
      "30         lag24_Energia    0.038896\n",
      "12           day_of_week    0.036212\n",
      "17              sin_24_1    0.024742\n",
      "21              sin_24_3    0.016810\n",
      "2               PrecEuro    0.009642\n",
      "6            Tipo_Unidad    0.000000\n",
      "3            Descripción    0.000000\n",
      "27      energia_hour_sin    0.000000\n",
      "26               dow_cos    0.000000\n",
      "25               dow_sin    0.000000\n",
      "24              hour_cos    0.000000\n",
      "23              hour_sin    0.000000\n",
      "22              cos_24_3    0.000000\n",
      "19              sin_24_2    0.000000\n",
      "20              cos_24_2    0.000000\n",
      "7          Zona/Frontera    0.000000\n",
      "18              cos_24_1    0.000000\n",
      "1            Descripcion    0.000000\n",
      "15            is_weekend    0.000000\n",
      "4                 Agente    0.000000\n",
      "13                 month    0.000000\n",
      "11                  hour    0.000000\n",
      "5   Porcentaje_Propiedad    0.000000\n",
      "8             Tecnología    0.000000\n",
      "16                     t    0.000000\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with feature names and their importance scores\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': FEATURES,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance in descending order\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-13 19:05:59,293] A new study created in memory with name: no-name-7cc3614a-041c-4ff7-9bd1-2be301f26054\n",
      "[I 2025-02-13 19:06:03,647] Trial 0 finished with value: 4.236947425301009 and parameters: {'model': 'XGB', 'xgb_n_estimators': 240, 'xgb_max_depth': 3, 'xgb_learning_rate': 0.13983740016490973, 'xgb_reg_alpha': 6.011150117432088, 'xgb_reg_lambda': 7.080725777960454, 'xgb_subsample': 0.5102922471479012, 'xgb_colsample_bytree': 0.9849549260809971}. Best is trial 0 with value: 4.236947425301009.\n",
      "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
      "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
      "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "[W 2025-02-13 19:06:40,457] Trial 1 failed with parameters: {'model': 'Prophet', 'prophet_changepoint_prior_scale': 0.026079656598095833, 'prophet_seasonality_prior_scale': 0.19762189340280073} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/var/folders/p1/vj5nk3dj08nbdgjg1lst8k_w0000gn/T/ipykernel_1568/2975525036.py\", line 55, in objective\n",
      "    y_pred = evaluate_prophet(prophet_model, test)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/p1/vj5nk3dj08nbdgjg1lst8k_w0000gn/T/ipykernel_1568/136044326.py\", line 27, in evaluate_prophet\n",
      "    forecast = prophet_model.predict(future)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/prophet/forecaster.py\", line 1275, in predict\n",
      "    intervals = self.predict_uncertainty(df, vectorized)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/prophet/forecaster.py\", line 1440, in predict_uncertainty\n",
      "    sim_values = self.sample_posterior_predictive(df, vectorized)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/prophet/forecaster.py\", line 1500, in sample_posterior_predictive\n",
      "    sim_values[k] = np.column_stack(v)\n",
      "                    ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/numpy/lib/shape_base.py\", line 652, in column_stack\n",
      "    return _nx.concatenate(arrays, 1)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-02-13 19:06:40,462] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[396], line 130\u001b[0m\n\u001b[1;32m    127\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m, sampler\u001b[38;5;241m=\u001b[39msampler)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Run the optimization (adjust n_trials as needed)\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Print out the best parameters and corresponding score\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[396], line 55\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     52\u001b[0m     prophet_model\u001b[38;5;241m.\u001b[39mfit(train_prophet)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_prophet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprophet_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGB\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# Hyperparameter search space for XGBoost\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     xgb_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxgb_n_estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1000\u001b[39m),\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m\"\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxgb_max_depth\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolsample_bytree\u001b[39m\u001b[38;5;124m\"\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxgb_colsample_bytree\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     67\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[387], line 27\u001b[0m, in \u001b[0;36mevaluate_prophet\u001b[0;34m(prophet_model, test)\u001b[0m\n\u001b[1;32m     24\u001b[0m future \u001b[38;5;241m=\u001b[39m test_prophet[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecEuro\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcum_energy\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Forecast\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m forecast \u001b[38;5;241m=\u001b[39m \u001b[43mprophet_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m forecast[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myhat\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/prophet/forecaster.py:1275\u001b[0m, in \u001b[0;36mProphet.predict\u001b[0;34m(self, df, vectorized)\u001b[0m\n\u001b[1;32m   1273\u001b[0m seasonal_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_seasonal_components(df)\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muncertainty_samples:\n\u001b[0;32m-> 1275\u001b[0m     intervals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_uncertainty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1277\u001b[0m     intervals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/prophet/forecaster.py:1440\u001b[0m, in \u001b[0;36mProphet.predict_uncertainty\u001b[0;34m(self, df, vectorized)\u001b[0m\n\u001b[1;32m   1428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict_uncertainty\u001b[39m(\u001b[38;5;28mself\u001b[39m, df: pd\u001b[38;5;241m.\u001b[39mDataFrame, vectorized: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m   1429\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prediction intervals for yhat and trend.\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m \n\u001b[1;32m   1431\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;124;03m    Dataframe with uncertainty intervals.\u001b[39;00m\n\u001b[1;32m   1439\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1440\u001b[0m     sim_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_posterior_predictive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1442\u001b[0m     lower_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterval_width) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m   1443\u001b[0m     upper_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterval_width) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/prophet/forecaster.py:1500\u001b[0m, in \u001b[0;36mProphet.sample_posterior_predictive\u001b[0;34m(self, df, vectorized)\u001b[0m\n\u001b[1;32m   1498\u001b[0m             sim_values[key]\u001b[38;5;241m.\u001b[39mappend(sim[key])\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m sim_values\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m-> 1500\u001b[0m     sim_values[k] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sim_values\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/numpy/lib/shape_base.py:652\u001b[0m, in \u001b[0;36mcolumn_stack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    650\u001b[0m         arr \u001b[38;5;241m=\u001b[39m array(arr, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, subok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, ndmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    651\u001b[0m     arrays\u001b[38;5;241m.\u001b[39mappend(arr)\n\u001b[0;32m--> 652\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================\n",
    "# Assumed pre-defined functions and variables:\n",
    "# --------------------------------------------\n",
    "# - data: your full DataFrame.\n",
    "# - FEATURES, TARGET: list of feature column names and target column name.\n",
    "# - tss: a time series cross-validation splitter.\n",
    "# - mae_score: a function to compute MAE.\n",
    "#\n",
    "# For each model, you must have implemented:\n",
    "#   Prophet: prepare_prophet(), train_prophet() [or inline training], evaluate_prophet()\n",
    "#   XGB: train_xgb(), evaluate_xgb()\n",
    "#   LGBM: train_lgbm(), evaluate_lgbm()\n",
    "#   TimesFM: prepare_timesfm(), train_timesfm(), evaluate_timesfm()\n",
    "#   LSTM: prepare_lstm(), train_lstm(), evaluate_lstm()\n",
    "# ============================================\n",
    "\n",
    "def objective(trial):\n",
    "    # Choose which model to tune in this trial.\n",
    "    model_name = trial.suggest_categorical(\"model\", [\"Prophet\", \"XGB\", \"LGBM\", \"TimesFM\", \"LSTM\"])\n",
    "    \n",
    "    fold_scores = []  # To store metric (e.g., MAE) for each CV fold\n",
    "    \n",
    "    for train_idx, test_idx in tss.split(data):\n",
    "        # Split the data for this fold\n",
    "        train = data.iloc[train_idx]\n",
    "        test = data.iloc[test_idx]\n",
    "        \n",
    "        # For models that require X and y explicitly.\n",
    "        X_train = train[FEATURES]\n",
    "        y_train = train[TARGET]\n",
    "        X_test = test[FEATURES]\n",
    "        y_test = test[TARGET]\n",
    "        \n",
    "        if model_name == \"Prophet\":\n",
    "            # Prepare Prophet data\n",
    "            train_prophet = prepare_prophet(train)\n",
    "            # Hyperparameters (regularization via changepoint and seasonality scales)\n",
    "            cp_scale = trial.suggest_float(\"prophet_changepoint_prior_scale\", 0.001, 0.5, log=True)\n",
    "            seas_scale = trial.suggest_float(\"prophet_seasonality_prior_scale\", 0.01, 10.0, log=True)\n",
    "            \n",
    "            # Initialize and train Prophet with extra regressors\n",
    "            from prophet import Prophet  # or fbprophet depending on your package\n",
    "            prophet_model = Prophet(changepoint_prior_scale=cp_scale,\n",
    "                                    seasonality_prior_scale=seas_scale)\n",
    "            prophet_model.add_regressor('PrecEuro')\n",
    "            prophet_model.add_regressor('cum_energy')\n",
    "            prophet_model.fit(train_prophet)\n",
    "            \n",
    "            # Evaluate\n",
    "            y_pred = evaluate_prophet(prophet_model, test)\n",
    "            \n",
    "        elif model_name == \"XGB\":\n",
    "            # Hyperparameter search space for XGBoost\n",
    "            xgb_params = {\n",
    "                \"n_estimators\": trial.suggest_int(\"xgb_n_estimators\", 100, 1000),\n",
    "                \"max_depth\": trial.suggest_int(\"xgb_max_depth\", 3, 10),\n",
    "                \"learning_rate\": trial.suggest_float(\"xgb_learning_rate\", 0.001, 0.3, log=True),\n",
    "                \"reg_alpha\": trial.suggest_float(\"xgb_reg_alpha\", 0.0, 10.0),\n",
    "                \"reg_lambda\": trial.suggest_float(\"xgb_reg_lambda\", 0.0, 10.0),\n",
    "                \"subsample\": trial.suggest_float(\"xgb_subsample\", 0.5, 1.0),\n",
    "                \"colsample_bytree\": trial.suggest_float(\"xgb_colsample_bytree\", 0.5, 1.0)\n",
    "            }\n",
    "            xgb_model = train_xgb(X_train, y_train, X_test, y_test, xgb_params)\n",
    "            y_pred = evaluate_xgb(xgb_model, X_test)\n",
    "            \n",
    "        elif model_name == \"LGBM\":\n",
    "            # Hyperparameter search space for LightGBM\n",
    "            lgbm_params = {\n",
    "                \"n_estimators\": trial.suggest_int(\"lgbm_n_estimators\", 100, 1000),\n",
    "                \"num_leaves\": trial.suggest_int(\"lgbm_num_leaves\", 20, 150),\n",
    "                \"learning_rate\": trial.suggest_float(\"lgbm_learning_rate\", 0.001, 0.3, log=True),\n",
    "                \"reg_alpha\": trial.suggest_float(\"lgbm_reg_alpha\", 0.0, 10.0),\n",
    "                \"reg_lambda\": trial.suggest_float(\"lgbm_reg_lambda\", 0.0, 10.0),\n",
    "                \"min_data_in_leaf\": trial.suggest_int(\"lgbm_min_data_in_leaf\", 10, 100)\n",
    "            }\n",
    "            lgbm_model = train_lgbm(X_train, y_train, X_test, y_test, lgbm_params)\n",
    "            y_pred = evaluate_lgbm(lgbm_model, X_test)\n",
    "            \n",
    "        elif model_name == \"TimesFM\":\n",
    "            # Prepare TimesFM data (assumed to have its own preparation method)\n",
    "            X_train_tf, y_train_tf, X_test_tf = prepare_timesfm(X_train, y_train, X_test)\n",
    "            # Hyperparameters for TimesFM model (example values)\n",
    "            timesfm_model_params = {\n",
    "                \"learning_rate\": trial.suggest_float(\"timesfm_learning_rate\", 0.001, 0.1, log=True),\n",
    "                \"reg_alpha\": trial.suggest_float(\"timesfm_reg_alpha\", 0.0, 10.0),\n",
    "                \"reg_lambda\": trial.suggest_float(\"timesfm_reg_lambda\", 0.0, 10.0)\n",
    "            }\n",
    "            timesfm_train_params = {\n",
    "                \"epochs\": trial.suggest_int(\"timesfm_epochs\", 10, 200)\n",
    "            }\n",
    "            timesfm_model = train_timesfm(X_train_tf, y_train_tf, timesfm_model_params, timesfm_train_params)\n",
    "            y_pred = evaluate_timesfm(timesfm_model, X_test_tf)\n",
    "            \n",
    "        elif model_name == \"LSTM\":\n",
    "            # Prepare LSTM data (using your own prepare_lstm)\n",
    "            X_train_lstm, y_train_lstm, X_test_lstm = prepare_lstm(X_train, y_train, X_test)\n",
    "            # Hyperparameter search space for LSTM (e.g., architecture + training reg params)\n",
    "            lstm_model_params = {\n",
    "                \"num_layers\": trial.suggest_int(\"lstm_num_layers\", 1, 3),\n",
    "                \"hidden_size\": trial.suggest_int(\"lstm_hidden_size\", 16, 128),\n",
    "                \"dropout\": trial.suggest_float(\"lstm_dropout\", 0.0, 0.5)\n",
    "            }\n",
    "            lstm_train_params = {\n",
    "                \"learning_rate\": trial.suggest_float(\"lstm_learning_rate\", 1e-5, 1e-2, log=True),\n",
    "                \"weight_decay\": trial.suggest_float(\"lstm_weight_decay\", 0.0, 0.1),\n",
    "                \"epochs\": trial.suggest_int(\"lstm_epochs\", 10, 200)\n",
    "            }\n",
    "            lstm_model = train_lstm(X_train_lstm, y_train_lstm, lstm_model_params, lstm_train_params)\n",
    "            y_pred = evaluate_lstm(lstm_model, X_test_lstm)\n",
    "        \n",
    "        # Calculate the evaluation metric (e.g., MAE) for this fold\n",
    "        fold_mae = mae_score(y_test, y_pred)\n",
    "        fold_scores.append(fold_mae)\n",
    "    \n",
    "    # Return the average score across folds (Optuna minimizes the objective)\n",
    "    return np.mean(fold_scores)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up Optuna with a TPE sampler (using a seed for reproducibility)\n",
    "    sampler = TPESampler(seed=42)\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "    \n",
    "    # Run the optimization (adjust n_trials as needed)\n",
    "    study.optimize(objective, n_trials=50)\n",
    "    \n",
    "    # Print out the best parameters and corresponding score\n",
    "    print(\"Best trial:\")\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"  MAE: {best_trial.value:.4f}\")\n",
    "    print(\"  Hyperparameters:\")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
