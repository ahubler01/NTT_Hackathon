{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from prophet import Prophet\n",
    "import timesfm\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import OrdinalEncoder, FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from numpy import fft\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --------------------------\n",
    "# Required Packages for Models\n",
    "# --------------------------\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# PyTorch and related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "cmdstanpy_logger = logging.getLogger(\"cmdstanpy\")\n",
    "absl_logger = logging.getLogger(\"absl\")\n",
    "cmdstanpy_logger.disabled = True\n",
    "absl_logger.disabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files: 100%|██████████| 3/3 [00:00<00:00, 56679.78it/s]\n"
     ]
    }
   ],
   "source": [
    "tfm = timesfm.TimesFm(\n",
    "      hparams=timesfm.TimesFmHparams(\n",
    "          backend=\"gpu\",\n",
    "          per_core_batch_size=32,\n",
    "          horizon_len=128,\n",
    "          num_layers=50,\n",
    "          use_positional_embedding=False,\n",
    "          context_len=2048,\n",
    "      ),\n",
    "      checkpoint=timesfm.TimesFmCheckpoint(\n",
    "          huggingface_repo_id=\"google/timesfm-2.0-500m-pytorch\"),\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omie_b = pd.read_csv('../../data/df_omie_blind(in).csv')\n",
    "df_omie_l = pd.read_csv('../../data/df_omie_labelled(in).csv')\n",
    "filtered_cat = pd.read_csv('../../data/filtered_categories(in).csv')\n",
    "unit_list = pd.read_csv('../../data/unit_list(in).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_omie_l.merge(unit_list, on='Codigo', how='left')\n",
    "data = data.merge(filtered_cat, on='Codigo', how='left')\n",
    "codes = filtered_cat['Codigo'].unique()\n",
    "data = data[data['Codigo'].isin(codes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 703248 entries, 0 to 712263\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   Codigo                703248 non-null  object \n",
      " 1   Descripcion           703248 non-null  object \n",
      " 2   fechaHora             703248 non-null  object \n",
      " 3   PrecEuro              703248 non-null  float64\n",
      " 4   Energia               703248 non-null  float64\n",
      " 5   Descripción           403466 non-null  object \n",
      " 6   Agente                403466 non-null  object \n",
      " 7   Porcentaje_Propiedad  403466 non-null  float64\n",
      " 8   Tipo_Unidad           403466 non-null  object \n",
      " 9   Zona/Frontera         403466 non-null  object \n",
      " 10  Tecnología            403466 non-null  object \n",
      " 11  Categoria             703248 non-null  object \n",
      "dtypes: float64(3), object(9)\n",
      "memory usage: 69.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_features(df: pd.DataFrame):\n",
    "    df['fechaHora'] = pd.to_datetime(df['fechaHora'])\n",
    "    df['date'] = df['fechaHora'].dt.date\n",
    "    df['hour'] = df['fechaHora'].dt.hour\n",
    "    df['day_of_week'] = df['fechaHora'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "    df['month'] = df['fechaHora'].dt.month\n",
    "    df['day_of_month'] = df['fechaHora'].dt.day\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    df.sort_values(['fechaHora', 'Codigo'], inplace=True)\n",
    "    df['t'] = (df['fechaHora'] - df['fechaHora'].min()).dt.total_seconds() / 3600\n",
    "    \n",
    "    def sin_cos_features(df: pd.DataFrame, period, K, time_col='t'):\n",
    "        df = df.sort_values(['Codigo', 'fechaHora'])\n",
    "        for k in range(1, K + 1):\n",
    "            df[f'sin_{period}_{k}'] = np.sin(2 * np.pi * k * df[time_col] / period)\n",
    "            df[f'cos_{period}_{k}'] = np.cos(2 * np.pi * k * df[time_col] / period)\n",
    "        return df\n",
    "    \n",
    "    df = sin_cos_features(df, period=24, K=3)\n",
    "    \n",
    "    return df    \n",
    "\n",
    "def cyclical_features(df: pd.DataFrame):\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    return df\n",
    "        \n",
    "def interaction_features(df: pd.DataFrame):\n",
    "    df['energia_hour_sin'] = df['Energia'] * df['hour_sin']\n",
    "    return df\n",
    "    \n",
    "def lags_features(df: pd.DataFrame):\n",
    "    df.sort_values(['fechaHora'], inplace=True)\n",
    "    df['lag_PrecEuro'] = df.groupby('Codigo')['PrecEuro'].shift(24*28)\n",
    "    df['lag_Energia'] = df.groupby('Codigo')['Energia'].shift(24*28)\n",
    "    df['lag_Energia'] = np.log(df['lag_Energia'] + 1)\n",
    "    df['lag1_Energia'] = df.groupby('Codigo')['lag_Energia'].shift(1)\n",
    "    df['lag24_Energia'] = df.groupby('Codigo')['lag_Energia'].shift(24)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(data: pd.DataFrame):\n",
    "    data['Energia_stationary'] = data['Energia'].diff()\n",
    "    data = time_features(data)\n",
    "    data = cyclical_features(data)\n",
    "    data = interaction_features(data)\n",
    "    data = lags_features(data)\n",
    "    data = data.sort_values(['fechaHora', 'Codigo'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = feature_engineering(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to lags, we have many missing values at the beginning of each codigo. As it represents a large proportion of the df, I will impute by the mean per hour. This might cause leakages for the first records but shouldn't impact the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descripción             299782\n",
      "Agente                  299782\n",
      "Porcentaje_Propiedad    299782\n",
      "Tipo_Unidad             299782\n",
      "Zona/Frontera           299782\n",
      "Tecnología              299782\n",
      "lag_PrecEuro            209664\n",
      "lag_Energia             209664\n",
      "lag1_Energia            209976\n",
      "lag24_Energia           217152\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nan_counts = data.isna().sum()\n",
    "nan_counts = nan_counts[nan_counts > 1]\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop Technologia, Zona/Frontera, Tipo_Unidad, Porcentaje_Propiedad, Agente and Descripción as their feature importance is close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['Descripción', 'Agente', 'Porcentaje_Propiedad', 'Tipo_Unidad', 'Zona/Frontera', 'Tecnología'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_codigo_hour(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Impute missing values in selected columns using the mean computed per Codigo and per hour.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    df['fechaHora'] = pd.to_datetime(df['fechaHora'])\n",
    "\n",
    "    cols_to_impute = ['lag_PrecEuro', 'lag_Energia', 'lag1_Energia', 'lag24_Energia']\n",
    "\n",
    "    for col in cols_to_impute:\n",
    "        df[col] = df.groupby(['Codigo', 'hour'])[col].transform(lambda x: x.fillna(x.mean()))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = impute_codigo_hour(data)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinalEncodingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        # Initialize the encoder with any desired options.\n",
    "        self.encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "        self.cat_cols = None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        # Identify categorical columns (dtype object) in the training data.\n",
    "        self.cat_cols = X.select_dtypes(include=['object']).columns\n",
    "        # Fit the encoder on the categorical columns.\n",
    "        self.encoder.fit(X[self.cat_cols])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X_transformed = X.copy()\n",
    "        if self.cat_cols is not None:\n",
    "            X_transformed[self.cat_cols] = self.encoder.transform(X_transformed[self.cat_cols])\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_features(df: pd.DataFrame):\n",
    "    df['cum_energy'] = df.groupby(['Codigo', 'date'])['lag_Energia'].cumsum()\n",
    "    return df\n",
    "\n",
    "def rolling_mean_features(df: pd.DataFrame):\n",
    "    features = ['lag_Energia', 'lag_PrecEuro']\n",
    "    groups = ['Codigo', 'Categoria']\n",
    "    times = [12, 24, 48, 168]\n",
    "    for group in groups:\n",
    "        for feature in features:\n",
    "            for time in times:\n",
    "                df[f'roll{time}_mean_{feature}'] = df.groupby(group)[feature] \\\n",
    "                    .transform(lambda x: x.rolling(window=time, min_periods=1).mean())\n",
    "    return df\n",
    "\n",
    "def ewm_features(df: pd.DataFrame):\n",
    "    features = ['lag_Energia', 'lag_PrecEuro']\n",
    "    groups = ['Codigo', 'Categoria']\n",
    "    spans = [12, 24, 48, 168]\n",
    "    for group in groups:\n",
    "        for feature in features:\n",
    "            for span in spans:\n",
    "                df[f'ewm{span}_mean_{feature}'] = df.groupby(group)[feature] \\\n",
    "                    .transform(lambda x: x.ewm(span=span, min_periods=1).mean())\n",
    "    return df\n",
    "\n",
    "def diff_features(df: pd.DataFrame):\n",
    "    features = ['lag_Energia', 'lag_PrecEuro']\n",
    "    for feature in features:\n",
    "        df[f'diff_{feature}'] = df.groupby('Codigo')[feature].diff()\n",
    "    return df\n",
    "\n",
    "def volatility_features(df: pd.DataFrame):\n",
    "    features = ['lag_Energia', 'lag_PrecEuro']\n",
    "    groups = ['Codigo', 'Categoria']\n",
    "    windows = [12, 24, 48, 168]\n",
    "    for group in groups:\n",
    "        for feature in features:\n",
    "            for window in windows:\n",
    "                df[f'volatility_{window}_{feature}'] = df.groupby(group)[feature] \\\n",
    "                    .transform(lambda x: x.rolling(window=window, min_periods=1).std())\n",
    "    return df\n",
    "\n",
    "def fourrier_features(df: pd.DataFrame):    \n",
    "    def apply_fft(group):\n",
    "            X = fft.fft(group['lag_Energia'])\n",
    "            N = len(X)\n",
    "            group['lag_Energia_fft'] = np.abs(X) / N  # Normalize by length\n",
    "            return group\n",
    "\n",
    "    df = df.groupby('Codigo', group_keys=False).apply(apply_fft)\n",
    "    return df\n",
    "\n",
    "def frequency_power_features(df: pd.DataFrame):\n",
    "    df['power_spectrum'] = df.groupby('Codigo')['lag_Energia'].transform(lambda x: np.abs(fft.fft(x))**2 / len(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('agg_features', FunctionTransformer(agg_features)),\n",
    "    ('rolling_mean_features', FunctionTransformer(rolling_mean_features)),\n",
    "    ('ewm_features', FunctionTransformer(ewm_features)),\n",
    "    ('diff_features', FunctionTransformer(diff_features)),\n",
    "    ('volatility_features', FunctionTransformer(volatility_features)),\n",
    "    ('fourrier_features', FunctionTransformer(fourrier_features)),\n",
    "    ('frequency_power_features', FunctionTransformer(frequency_power_features)),\n",
    "    ('ordinal_encoding', OrdinalEncodingTransformer()),\n",
    "    ('dropna', FunctionTransformer(lambda x: x.dropna()))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TimesFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimesFMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(TimesFMModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_timesfm(X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame) -> tuple:\n",
    "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "    return X_train_tensor, y_train_tensor, X_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESFM_MODEL_PARAMS = {\n",
    "    'hidden_dim': 100,\n",
    "    'output_dim': 1,\n",
    "}\n",
    "\n",
    "TIMESFM_TRAIN_PARAMS = {\n",
    "    'lr': 0.01,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_timesfm(\n",
    "    X_train_tensor: torch.Tensor, \n",
    "    y_train_tensor: torch.Tensor,\n",
    "    TIMESFM_MODEL_PARAMS: dict,\n",
    "    TIMESFM_TRAIN_PARAMS: dict,\n",
    "    epochs: int = 150\n",
    "    ) -> TimesFMModel:\n",
    "\n",
    "    timesfm_model = TimesFMModel(input_dim=X_train_tensor.shape[1], **TIMESFM_MODEL_PARAMS)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(timesfm_model.parameters(), **TIMESFM_TRAIN_PARAMS)\n",
    "    \n",
    "    # Train the model.\n",
    "    timesfm_model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = timesfm_model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return timesfm_model\n",
    "    \n",
    "def evaluate_timesfm(timesfm_model: TimesFMModel, X_test_tensor: torch.Tensor) -> np.ndarray:\n",
    "    timesfm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_timesfm = timesfm_model(X_test_tensor).numpy().flatten()\n",
    "        \n",
    "    return y_pred_timesfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # batch_first=True makes input shape (batch, seq, feature)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states with zeros (and send to same device as x)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        # Use the output from the final time step\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lstm(X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame) -> tuple:\n",
    "    selected_features = [\n",
    "        'lag1_Energia', 'lag24_Energia', 'roll24_mean_Energia',\n",
    "        'hour', 'day_of_week', 'day_of_month', 'month', 'is_weekend',\n",
    "        'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos',\n",
    "        'PrecEuro', 'cum_energy'\n",
    "    ]\n",
    "    X_train_selected = X_train[selected_features].values  # shape: (num_samples, num_features)\n",
    "    X_test_selected  = X_test[selected_features].values\n",
    "\n",
    "    # Add a time dimension (sequence length = 1)\n",
    "    X_train_lstm = torch.tensor(X_train_selected, dtype=torch.float32).unsqueeze(1)  # shape: (samples, 1, num_features)\n",
    "    X_test_lstm  = torch.tensor(X_test_selected, dtype=torch.float32).unsqueeze(1)\n",
    "    y_train_lstm = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "    \n",
    "    return X_train_lstm, y_train_lstm, X_test_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_MODEL_PARAMS = {\n",
    "    'hidden_size': 50,\n",
    "    'num_layers': 1,\n",
    "    'output_size': 1\n",
    "}\n",
    "\n",
    "LSTM_TRAIN_PARAMS = {\n",
    "    'lr': 0.01,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(\n",
    "    X_train: torch.Tensor, \n",
    "    y_train: torch.Tensor, \n",
    "    LSTM_MODEL_PARAMS: dict,\n",
    "    LSTM_TRAIN_PARAMS: dict,\n",
    "    epochs: int=150\n",
    "    ) -> LSTMModel:\n",
    "    \n",
    "    input_size = X_train.shape[2]\n",
    "    lstm_model = LSTMModel(input_size, **LSTM_MODEL_PARAMS)\n",
    "\n",
    "    criterion_lstm = nn.MSELoss()\n",
    "    optimizer_lstm = optim.Adam(lstm_model.parameters(), **LSTM_TRAIN_PARAMS)\n",
    "    \n",
    "    lstm_model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer_lstm.zero_grad()\n",
    "        outputs = lstm_model(X_train)\n",
    "        loss = criterion_lstm(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer_lstm.step()\n",
    "        \n",
    "    return lstm_model\n",
    "    \n",
    "    \n",
    "def evaluate_lstm(lstm_model: LSTMModel, X_test_lstm: torch.Tensor) -> np.ndarray:\n",
    "    lstm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_lstm = lstm_model(X_test_lstm).numpy().flatten()\n",
    "    return y_pred_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_PARAMS = {\n",
    "    'n_estimators': 1000,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.01,\n",
    "    'verbosity': 0,\n",
    "    'booster': 'gbtree'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgb(\n",
    "    X_train: pd.DataFrame, \n",
    "    y_train: pd.Series, \n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    XGB_PARAMS: dict) -> xgb.XGBRegressor:    \n",
    "    xgb_reg = xgb.XGBRegressor(**XGB_PARAMS)\n",
    "    xgb_reg.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        verbose=False\n",
    "    )\n",
    "    return xgb_reg\n",
    "    \n",
    "def evaluate_xgb(xgb_reg: xgb.XGBRegressor, X_test: pd.DataFrame) -> np.ndarray:\n",
    "    return xgb_reg.predict(X_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBM_PARAMS = {\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm(\n",
    "    X_train: pd.DataFrame, \n",
    "    y_train: pd.Series, \n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    LGBM_PARAMS: dict\n",
    "    ) -> LGBMRegressor:    \n",
    "    lgbm_reg = LGBMRegressor(**LGBM_PARAMS)\n",
    "    lgbm_reg.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)]\n",
    "    )\n",
    "    return lgbm_reg\n",
    "    \n",
    "def evaluate_lgbm(lgbm_reg: LGBMRegressor, X_test: pd.DataFrame) -> np.ndarray:\n",
    "    return lgbm_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prophet(train: pd.DataFrame, TARGET: str) -> pd.DataFrame:\n",
    "    train_prophet = train.reset_index()\n",
    "    train_prophet['ds'] = pd.to_datetime(train_prophet['fechaHora'])\n",
    "\n",
    "    train_prophet = train_prophet.rename(columns={TARGET: 'y'})\n",
    "\n",
    "    return train_prophet[['ds', 'y', 'lag_PrecEuro', 'lag_Energia']]\n",
    "\n",
    "def train_prophet(train_prophet: pd.DataFrame) -> Prophet:\n",
    "\n",
    "    prophet_model = Prophet()\n",
    "    prophet_model.add_regressor('lag_PrecEuro')\n",
    "    prophet_model.add_regressor('lag_Energia')\n",
    "\n",
    "    # Fit the model\n",
    "    prophet_model.fit(train_prophet)\n",
    "    \n",
    "    return prophet_model\n",
    "\n",
    "def evaluate_prophet(prophet_model: Prophet, test: pd.DataFrame) -> np.ndarray:\n",
    "\n",
    "    test_prophet = test.reset_index()\n",
    "    test_prophet['ds'] = pd.to_datetime(test_prophet['fechaHora'])\n",
    "    future = test_prophet[['ds', 'lag_PrecEuro', 'lag_Energia']]\n",
    "\n",
    "    # Forecast\n",
    "    forecast = prophet_model.predict(future)\n",
    "    return forecast['yhat'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Rolling Window CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_rolling_window_cv(data: pd.DataFrame, initial_train_window: int, forecast_horizon: int, step: int):\n",
    "    \"\"\"\n",
    "    Custom rolling window cross-validation.\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    train_end = initial_train_window  \n",
    "    while (train_end + forecast_horizon) <= n:\n",
    "        train_idx = list(range(0, train_end))\n",
    "        test_idx = list(range(train_end, train_end + forecast_horizon))\n",
    "        yield train_idx, test_idx\n",
    "        train_end += step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "nunique_codes = data['Codigo'].nunique()\n",
    "INITIAL_TRAIN_WINDOW = 24*28*nunique_codes\n",
    "FORECAST_HORIZON = 24*28*nunique_codes\n",
    "STEP = 24*7*nunique_codes\n",
    "\n",
    "EXCLUDED_COLS = ['fechaHora', 'Energia', 'Energia_stationary']\n",
    "FEATURES = [col for col in data.columns if col not in EXCLUDED_COLS]\n",
    "TARGET = 'Energia_stationary'\n",
    "\n",
    "model_names = ['TimesFM', 'XGB', 'LGBM', 'Prophet', 'LSTM']\n",
    "results = {model: {'mae': [], 'mape': []} for model in model_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_score(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "def mape_score(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / np.abs(y_true) + 1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (463943, 29)\n",
      "X_train shape: (463631, 55)\n",
      "test shape: (239304, 29)\n",
      "X_test shape: (238992, 55)\n"
     ]
    }
   ],
   "source": [
    "# Split the data such that the test set represents the last 3 months\n",
    "split_date = pd.to_datetime('2024-05-01')\n",
    "train = data[data['fechaHora'] < split_date]\n",
    "test = data[data['fechaHora'] >= split_date]\n",
    "\n",
    "EXCLUDED_COLS = ['fechaHora', 'Energia', 'Energia_stationary']\n",
    "FEATURES = [col for col in data.columns if col not in EXCLUDED_COLS]\n",
    "TARGET = 'Energia_stationary'\n",
    "\n",
    "# Common splits for models that require X and y inputs:\n",
    "X_train = train[FEATURES]\n",
    "y_train = train[TARGET]\n",
    "X_test  = test[FEATURES]\n",
    "y_test  = test[TARGET]\n",
    "\n",
    "X_train = pipeline.fit_transform(X_train)\n",
    "X_test = pipeline.transform(X_test)\n",
    "\n",
    "print(f\"train shape: {train.shape}\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"test shape: {test.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 0 =====\n",
      "X_train_timesfm tensor([[  1.0000, 113.0000,   0.0000,  ...,  19.6249,   0.0000,   0.0000],\n",
      "        [  2.0000, 214.0000,   0.0000,  ...,  19.5621,   0.0000,   0.0000],\n",
      "        [  3.0000, 217.0000,   0.0000,  ...,  19.4998,   0.0000,   0.0000],\n",
      "        ...,\n",
      "        [310.0000, 300.0000,  -1.0000,  ...,  27.7208,   0.0000,   0.0000],\n",
      "        [311.0000, 301.0000,   0.0000,  ...,  27.7200,   0.0000,   0.0000],\n",
      "        [  0.0000, 177.0000,   0.0000,  ...,  19.4434,   0.0000,   0.0000]])\n",
      "y_train_timesfm tensor([[  0.0000],\n",
      "        [ 46.2000],\n",
      "        [-46.2000],\n",
      "        ...,\n",
      "        [ 14.9000],\n",
      "        [ 51.8000],\n",
      "        [-67.0000]])\n",
      "X_test_timesfm tensor([[ 1.0000e+00,  1.1300e+02,  0.0000e+00,  ...,  2.2840e+00,\n",
      "          8.6599e-02,  5.0396e+00],\n",
      "        [ 2.0000e+00,  2.1400e+02,  0.0000e+00,  ...,  2.2767e+00,\n",
      "          9.5149e-02,  6.0839e+00],\n",
      "        [ 3.0000e+00,  2.1700e+02,  0.0000e+00,  ...,  2.2694e+00,\n",
      "          4.5533e-02,  1.3932e+00],\n",
      "        ...,\n",
      "        [ 3.1000e+02,  3.0000e+02, -1.0000e+01,  ...,  5.4640e+01,\n",
      "          3.1629e-01,  6.7225e+01],\n",
      "        [ 3.1100e+02,  3.0100e+02,  0.0000e+00,  ...,  5.4640e+01,\n",
      "          2.7321e-01,  5.0159e+01],\n",
      "        [ 0.0000e+00,  1.7700e+02,  0.0000e+00,  ...,  7.7200e+01,\n",
      "          5.3788e-02,  1.9442e+00]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (209352) must match the size of tensor b (209664) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[756], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_train_timesfm \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_train_timesfm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_test_timesfm \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test_timesfm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 67\u001b[0m timesfm_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_timesfm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_timesfm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_timesfm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTIMESFM_MODEL_PARAMS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTIMESFM_TRAIN_PARAMS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m y_pred_timesfm \u001b[38;5;241m=\u001b[39m evaluate_timesfm(timesfm_model, X_test_timesfm)\n\u001b[1;32m     69\u001b[0m mae_timesfm \u001b[38;5;241m=\u001b[39m mae_score(y_test, y_pred_timesfm)\n",
      "Cell \u001b[0;32mIn[748], line 18\u001b[0m, in \u001b[0;36mtrain_timesfm\u001b[0;34m(X_train_tensor, y_train_tensor, TIMESFM_MODEL_PARAMS, TIMESFM_TRAIN_PARAMS, epochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m timesfm_model(X_train_tensor)\n\u001b[0;32m---> 18\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/torch/nn/modules/loss.py:608\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/torch/nn/functional.py:3791\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3789\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3791\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(\n\u001b[1;32m   3793\u001b[0m     expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3794\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/torch/functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (209352) must match the size of tensor b (209664) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "model_names = ['TimesFM']\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(custom_rolling_window_cv(data, INITIAL_TRAIN_WINDOW, FORECAST_HORIZON, STEP)):\n",
    "    print(f\"\\n===== Fold {fold} =====\")\n",
    "    \n",
    "    # Split the data into train and test folds\n",
    "    train = data.iloc[train_idx]\n",
    "    test = data.iloc[test_idx]\n",
    "    \n",
    "    # Common splits for models that require X and y inputs:\n",
    "    X_train = train[FEATURES]\n",
    "    y_train = train[TARGET]\n",
    "    X_test  = test[FEATURES]\n",
    "    y_test  = test[TARGET]\n",
    "    \n",
    "    X_train = pipeline.fit_transform(X_train)\n",
    "    X_test = pipeline.transform(X_test)\n",
    "    \n",
    "    # --------------------------\n",
    "    if 'Prophet' in model_names:\n",
    "        data_prophet = prepare_prophet(train, TARGET)\n",
    "        prophet_model = train_prophet(data_prophet)\n",
    "        y_pred_prophet = evaluate_prophet(prophet_model, test)\n",
    "        \n",
    "        mae_prophet = mae_score(y_test, y_pred_prophet)\n",
    "        mape_prophet = mape_score(y_test, y_pred_prophet)\n",
    "        \n",
    "        results['Prophet']['mae'].append(mae_score(y_test, y_pred_prophet))\n",
    "        results['Prophet']['mape'].append(mape_prophet)\n",
    "        \n",
    "        print(f\"Prophet    --> MAE: {mae_prophet:.4f}, MAPE: {mape_prophet:.4f}\")\n",
    "    \n",
    "    # --------------------------\n",
    "    if 'XGB' in model_names:\n",
    "        xgb_model = train_xgb(X_train, y_train, X_test, y_test, XGB_PARAMS)\n",
    "        y_pred_xgb = evaluate_xgb(xgb_model, X_test)\n",
    "        \n",
    "        mae_xgb = mae_score(y_test, y_pred_xgb)\n",
    "        mape_xgb = mape_score(y_test, y_pred_xgb)\n",
    "        \n",
    "        results['XGB']['mae'].append(mae_xgb)\n",
    "        results['XGB']['mape'].append(mape_xgb)\n",
    "        \n",
    "        print(f\"XGBoost    --> MAE: {mae_xgb:.4f}, MAPE: {mape_xgb:.4f}\")\n",
    "    \n",
    "    # --------------------------\n",
    "    if 'LGBM' in model_names:\n",
    "        lgbm_model = train_lgbm(X_train, y_train,  X_test, y_test, LGBM_PARAMS)\n",
    "        y_pred_lgbm = evaluate_lgbm(lgbm_model, X_test)\n",
    "\n",
    "        mae_lgbm = mae_score(y_test, y_pred_lgbm)\n",
    "        mape_lgbm = mape_score(y_test, y_pred_lgbm)\n",
    "\n",
    "        results['LGBM']['mae'].append(mae_lgbm)\n",
    "        results['LGBM']['mape'].append(mape_lgbm)\n",
    "\n",
    "        print(f\"LightGBM   --> MAE: {mae_lgbm:.4f}, MAPE: {mape_lgbm:.4f}\")\n",
    "\n",
    "    # --------------------------\n",
    "    if 'TimesFM' in model_names:\n",
    "        X_train_timesfm, y_train_timesfm, X_test_timesfm = prepare_timesfm(X_train, y_train, X_test)  \n",
    "        \n",
    "        print(f\"X_train_timesfm {X_train_timesfm}\")\n",
    "        print(f\"y_train_timesfm {y_train_timesfm}\")\n",
    "        print(f\"X_test_timesfm {X_test_timesfm}\")\n",
    "        \n",
    "        timesfm_model = train_timesfm(X_train_timesfm, y_train_timesfm, TIMESFM_MODEL_PARAMS, TIMESFM_TRAIN_PARAMS)\n",
    "        y_pred_timesfm = evaluate_timesfm(timesfm_model, X_test_timesfm)\n",
    "        mae_timesfm = mae_score(y_test, y_pred_timesfm)\n",
    "        mape_timesfm = mape_score(y_test, y_pred_timesfm)\n",
    "        \n",
    "        results['TimesFM']['mae'].append(mae_timesfm)\n",
    "        results['TimesFM']['mape'].append(mape_timesfm)\n",
    "        \n",
    "        print(f\"TimesFM   --> MAE: {mae_timesfm:.4f}, MAPE: {mape_timesfm:.4f}\")\n",
    "    \n",
    "    # --------------------------\n",
    "    # LSTM Model (PyTorch)\n",
    "    # --------------------------\n",
    "    if 'LSTM' in model_names:\n",
    "        X_train_lstm, y_train_lstm, X_test_timesfm = prepare_lstm(X_train, y_train, X_test)\n",
    "        lstm_model = train_lstm(X_train_lstm, y_train_lstm, LSTM_MODEL_PARAMS, LSTM_TRAIN_PARAMS)\n",
    "        y_pred_lstm = evaluate_lstm(lstm_model, X_test_timesfm)\n",
    "        \n",
    "        mae_lstm = mae_score(y_test, y_pred_lstm)\n",
    "        mape_lstm = mape_score(y_test, y_pred_lstm)\n",
    "        \n",
    "        results['LSTM']['mae'].append(mae_lstm)\n",
    "        results['LSTM']['mape'].append(mape_lstm)\n",
    "        \n",
    "        print(f\"LSTM       --> MAE: {mae_lstm:.4f}, MAPE: {mape_lstm:.4f}\")\n",
    "    \n",
    "# --------------------------\n",
    "# STEP 5: Aggregate and Display Results\n",
    "# --------------------------\n",
    "benchmark_results = []\n",
    "for model in model_names:\n",
    "    avg_mae = np.mean(results[model]['mae'])\n",
    "    avg_mape = np.mean(results[model]['mape'])\n",
    "    benchmark_results.append({\n",
    "        'Model': model,\n",
    "        'MAE': avg_mae,\n",
    "        'MAPE': avg_mape\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(benchmark_results)\n",
    "print(\"\\n===== Benchmark Results =====\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost    --> MAE: 36.6586, MAPE: inf\n"
     ]
    }
   ],
   "source": [
    "XGB_PARAMS = {\n",
    "    'n_estimators': 1000,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.01,\n",
    "    'verbosity': 0,\n",
    "    'booster': 'gbtree'\n",
    "}\n",
    "\n",
    "# Convert 'fechaHora' column to datetime\n",
    "data['fechaHora'] = pd.to_datetime(data['fechaHora'])\n",
    "\n",
    "# Split the data such that the test set represents the last 3 months\n",
    "split_date = pd.to_datetime('2024-05-01')\n",
    "train = data[data['fechaHora'] < split_date]\n",
    "test = data[data['fechaHora'] >= split_date]\n",
    "\n",
    "EXCLUDED_COLS = ['fechaHora', 'Energia', 'Energia_stationary']\n",
    "FEATURES = [col for col in data.columns if col not in EXCLUDED_COLS]\n",
    "TARGET = 'Energia_stationary'\n",
    "\n",
    "# Common splits for models that require X and y inputs:\n",
    "X_train = train[FEATURES]\n",
    "y_train = train[TARGET]\n",
    "X_test  = test[FEATURES]\n",
    "y_test  = test[TARGET]\n",
    "\n",
    "X_train = pipeline.fit_transform(X_train)\n",
    "X_test = pipeline.transform(X_test)\n",
    "\n",
    "xgb_model = train_xgb(X_train, y_train, X_test, y_test, XGB_PARAMS)\n",
    "y_pred_xgb = evaluate_xgb(xgb_model, X_test)\n",
    "\n",
    "mae_xgb = mae_score(y_test, y_pred_xgb)\n",
    "mape_xgb = mape_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"XGBoost    --> MAE: {mae_xgb:.4f}, MAPE: {mape_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Feature  Importance\n",
      "25  roll24_mean_Energia    0.410571\n",
      "0                Codigo    0.105280\n",
      "1           Descripcion    0.082795\n",
      "24        lag24_Energia    0.080454\n",
      "22          lag_Energia    0.062449\n",
      "2              PrecEuro    0.049419\n",
      "10                    t    0.043154\n",
      "3            cum_energy    0.037681\n",
      "6           day_of_week    0.029406\n",
      "4                  date    0.019509\n",
      "21     energia_hour_sin    0.018123\n",
      "8          day_of_month    0.016967\n",
      "5                  hour    0.014971\n",
      "12             cos_24_1    0.010592\n",
      "11             sin_24_1    0.006584\n",
      "14             cos_24_2    0.005432\n",
      "23         lag1_Energia    0.005317\n",
      "19              dow_sin    0.001294\n",
      "16             cos_24_3    0.000000\n",
      "17             hour_sin    0.000000\n",
      "18             hour_cos    0.000000\n",
      "20              dow_cos    0.000000\n",
      "15             sin_24_3    0.000000\n",
      "9            is_weekend    0.000000\n",
      "7                 month    0.000000\n",
      "13             sin_24_2    0.000000\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with feature names and their importance scores\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': FEATURES,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance in descending order\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-13 22:12:58,419] A new study created in memory with name: no-name-0414fe39-42f3-43e1-a01a-900313c5888e\n",
      "[W 2025-02-13 22:12:58,421] Trial 0 failed with parameters: {'model': 'XGB'} because of the following error: ValueError('Too many splits=5 for number of samples=703246 with test_size=209664 and gap=24.').\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/var/folders/p1/vj5nk3dj08nbdgjg1lst8k_w0000gn/T/ipykernel_1568/2975525036.py\", line 28, in objective\n",
      "    for train_idx, test_idx in tss.split(data):\n",
      "  File \"/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/sklearn/model_selection/_split.py\", line 1252, in _split\n",
      "    raise ValueError(\n",
      "ValueError: Too many splits=5 for number of samples=703246 with test_size=209664 and gap=24.\n",
      "[W 2025-02-13 22:12:58,422] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Too many splits=5 for number of samples=703246 with test_size=209664 and gap=24.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[649], line 130\u001b[0m\n\u001b[1;32m    127\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m, sampler\u001b[38;5;241m=\u001b[39msampler)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Run the optimization (adjust n_trials as needed)\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Print out the best parameters and corresponding score\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[649], line 28\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     24\u001b[0m model_name \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_categorical(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProphet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGB\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLGBM\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimesFM\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     26\u001b[0m fold_scores \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# To store metric (e.g., MAE) for each CV fold\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Split the data for this fold\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_idx\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_env/lib/python3.11/site-packages/sklearn/model_selection/_split.py:1252\u001b[0m, in \u001b[0;36mTimeSeriesSplit._split\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have number of folds=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_folds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m greater\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1250\u001b[0m     )\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m-\u001b[39m gap \u001b[38;5;241m-\u001b[39m (test_size \u001b[38;5;241m*\u001b[39m n_splits) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1253\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToo many splits=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_splits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for number of samples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1254\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with test_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and gap=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgap\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1255\u001b[0m     )\n\u001b[1;32m   1257\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(n_samples)\n\u001b[1;32m   1258\u001b[0m test_starts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(n_samples \u001b[38;5;241m-\u001b[39m n_splits \u001b[38;5;241m*\u001b[39m test_size, n_samples, test_size)\n",
      "\u001b[0;31mValueError\u001b[0m: Too many splits=5 for number of samples=703246 with test_size=209664 and gap=24."
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================\n",
    "# Assumed pre-defined functions and variables:\n",
    "# --------------------------------------------\n",
    "# - data: your full DataFrame.\n",
    "# - FEATURES, TARGET: list of feature column names and target column name.\n",
    "# - tss: a time series cross-validation splitter.\n",
    "# - mae_score: a function to compute MAE.\n",
    "#\n",
    "# For each model, you must have implemented:\n",
    "#   Prophet: prepare_prophet(), train_prophet() [or inline training], evaluate_prophet()\n",
    "#   XGB: train_xgb(), evaluate_xgb()\n",
    "#   LGBM: train_lgbm(), evaluate_lgbm()\n",
    "#   TimesFM: prepare_timesfm(), train_timesfm(), evaluate_timesfm()\n",
    "#   LSTM: prepare_lstm(), train_lstm(), evaluate_lstm()\n",
    "# ============================================\n",
    "\n",
    "def objective(trial):\n",
    "    # Choose which model to tune in this trial.\n",
    "    model_name = trial.suggest_categorical(\"model\", [\"Prophet\", \"XGB\", \"LGBM\", \"TimesFM\", \"LSTM\"])\n",
    "    \n",
    "    fold_scores = []  # To store metric (e.g., MAE) for each CV fold\n",
    "    \n",
    "    for train_idx, test_idx in tss.split(data):\n",
    "        # Split the data for this fold\n",
    "        train = data.iloc[train_idx]\n",
    "        test = data.iloc[test_idx]\n",
    "        \n",
    "        # For models that require X and y explicitly.\n",
    "        X_train = train[FEATURES]\n",
    "        y_train = train[TARGET]\n",
    "        X_test = test[FEATURES]\n",
    "        y_test = test[TARGET]\n",
    "        \n",
    "        if model_name == \"Prophet\":\n",
    "            # Prepare Prophet data\n",
    "            train_prophet = prepare_prophet(train)\n",
    "            # Hyperparameters (regularization via changepoint and seasonality scales)\n",
    "            cp_scale = trial.suggest_float(\"prophet_changepoint_prior_scale\", 0.001, 0.5, log=True)\n",
    "            seas_scale = trial.suggest_float(\"prophet_seasonality_prior_scale\", 0.01, 10.0, log=True)\n",
    "            \n",
    "            # Initialize and train Prophet with extra regressors\n",
    "            from prophet import Prophet  # or fbprophet depending on your package\n",
    "            prophet_model = Prophet(changepoint_prior_scale=cp_scale,\n",
    "                                    seasonality_prior_scale=seas_scale)\n",
    "            prophet_model.add_regressor('PrecEuro')\n",
    "            prophet_model.add_regressor('cum_energy')\n",
    "            prophet_model.fit(train_prophet)\n",
    "            \n",
    "            # Evaluate\n",
    "            y_pred = evaluate_prophet(prophet_model, test)\n",
    "            \n",
    "        elif model_name == \"XGB\":\n",
    "            # Hyperparameter search space for XGBoost\n",
    "            xgb_params = {\n",
    "                \"n_estimators\": trial.suggest_int(\"xgb_n_estimators\", 100, 1000),\n",
    "                \"max_depth\": trial.suggest_int(\"xgb_max_depth\", 3, 10),\n",
    "                \"learning_rate\": trial.suggest_float(\"xgb_learning_rate\", 0.001, 0.3, log=True),\n",
    "                \"reg_alpha\": trial.suggest_float(\"xgb_reg_alpha\", 0.0, 10.0),\n",
    "                \"reg_lambda\": trial.suggest_float(\"xgb_reg_lambda\", 0.0, 10.0),\n",
    "                \"subsample\": trial.suggest_float(\"xgb_subsample\", 0.5, 1.0),\n",
    "                \"colsample_bytree\": trial.suggest_float(\"xgb_colsample_bytree\", 0.5, 1.0)\n",
    "            }\n",
    "            xgb_model = train_xgb(X_train, y_train, X_test, y_test, xgb_params)\n",
    "            y_pred = evaluate_xgb(xgb_model, X_test)\n",
    "            \n",
    "        elif model_name == \"LGBM\":\n",
    "            # Hyperparameter search space for LightGBM\n",
    "            lgbm_params = {\n",
    "                \"n_estimators\": trial.suggest_int(\"lgbm_n_estimators\", 100, 1000),\n",
    "                \"num_leaves\": trial.suggest_int(\"lgbm_num_leaves\", 20, 150),\n",
    "                \"learning_rate\": trial.suggest_float(\"lgbm_learning_rate\", 0.001, 0.3, log=True),\n",
    "                \"reg_alpha\": trial.suggest_float(\"lgbm_reg_alpha\", 0.0, 10.0),\n",
    "                \"reg_lambda\": trial.suggest_float(\"lgbm_reg_lambda\", 0.0, 10.0),\n",
    "                \"min_data_in_leaf\": trial.suggest_int(\"lgbm_min_data_in_leaf\", 10, 100)\n",
    "            }\n",
    "            lgbm_model = train_lgbm(X_train, y_train, X_test, y_test, lgbm_params)\n",
    "            y_pred = evaluate_lgbm(lgbm_model, X_test)\n",
    "            \n",
    "        elif model_name == \"TimesFM\":\n",
    "            # Prepare TimesFM data (assumed to have its own preparation method)\n",
    "            X_train_tf, y_train_tf, X_test_tf = prepare_timesfm(X_train, y_train, X_test)\n",
    "            # Hyperparameters for TimesFM model (example values)\n",
    "            timesfm_model_params = {\n",
    "                \"learning_rate\": trial.suggest_float(\"timesfm_learning_rate\", 0.001, 0.1, log=True),\n",
    "                \"reg_alpha\": trial.suggest_float(\"timesfm_reg_alpha\", 0.0, 10.0),\n",
    "                \"reg_lambda\": trial.suggest_float(\"timesfm_reg_lambda\", 0.0, 10.0)\n",
    "            }\n",
    "            timesfm_train_params = {\n",
    "                \"epochs\": trial.suggest_int(\"timesfm_epochs\", 10, 200)\n",
    "            }\n",
    "            timesfm_model = train_timesfm(X_train_tf, y_train_tf, timesfm_model_params, timesfm_train_params)\n",
    "            y_pred = evaluate_timesfm(timesfm_model, X_test_tf)\n",
    "            \n",
    "        elif model_name == \"LSTM\":\n",
    "            # Prepare LSTM data (using your own prepare_lstm)\n",
    "            X_train_lstm, y_train_lstm, X_test_lstm = prepare_lstm(X_train, y_train, X_test)\n",
    "            # Hyperparameter search space for LSTM (e.g., architecture + training reg params)\n",
    "            lstm_model_params = {\n",
    "                \"num_layers\": trial.suggest_int(\"lstm_num_layers\", 1, 3),\n",
    "                \"hidden_size\": trial.suggest_int(\"lstm_hidden_size\", 16, 128),\n",
    "                \"dropout\": trial.suggest_float(\"lstm_dropout\", 0.0, 0.5)\n",
    "            }\n",
    "            lstm_train_params = {\n",
    "                \"learning_rate\": trial.suggest_float(\"lstm_learning_rate\", 1e-5, 1e-2, log=True),\n",
    "                \"weight_decay\": trial.suggest_float(\"lstm_weight_decay\", 0.0, 0.1),\n",
    "                \"epochs\": trial.suggest_int(\"lstm_epochs\", 10, 200)\n",
    "            }\n",
    "            lstm_model = train_lstm(X_train_lstm, y_train_lstm, lstm_model_params, lstm_train_params)\n",
    "            y_pred = evaluate_lstm(lstm_model, X_test_lstm)\n",
    "        \n",
    "        # Calculate the evaluation metric (e.g., MAE) for this fold\n",
    "        fold_mae = mae_score(y_test, y_pred)\n",
    "        fold_scores.append(fold_mae)\n",
    "    \n",
    "    # Return the average score across folds (Optuna minimizes the objective)\n",
    "    return np.mean(fold_scores)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up Optuna with a TPE sampler (using a seed for reproducibility)\n",
    "    sampler = TPESampler(seed=42)\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "    \n",
    "    # Run the optimization (adjust n_trials as needed)\n",
    "    study.optimize(objective, n_trials=50)\n",
    "    \n",
    "    # Print out the best parameters and corresponding score\n",
    "    print(\"Best trial:\")\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"  MAE: {best_trial.value:.4f}\")\n",
    "    print(\"  Hyperparameters:\")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
